# src/train.py
import argparse
import csv
import os
import random

import numpy as np
import pandas as pd
import torch
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader

# å¾ src è³‡æ–™å¤¾ä¸­å¼•å…¥æˆ‘å€‘å¯«å¥½çš„æ¨¡çµ„
from src.dataset import PigDataset
from src.engine import evaluate, train_one_epoch
from src.model import create_model
from src.transforms import get_transform
from src.utils import collate_fn  # âœ… ç›´æ¥åŒ¯å…¥å‡½å¼æœ¬é«”

# --- å…¨åŸŸå¸¸æ•¸ ---
DEVICE = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
NUM_CLASSES = 2  # 1 (pig) + 1 (background)


def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def seed_worker(_):
    # è®“ DataLoader workers çš„äº‚æ•¸å¯é‡ç¾
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def main():
    # --- 1. è¨­å®šèˆ‡è§£æå‘½ä»¤è¡Œåƒæ•¸ ---
    parser = argparse.ArgumentParser(description="Pig Detection Training Script")
    default_dr = "/content/data" if os.path.exists("/content/data") else "./data"
    parser.add_argument("--data_root", type=str, default=default_dr, help="Root path that contains train/ and test/")
    parser.add_argument("--epochs", type=int, default=30, help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for training")
    parser.add_argument("--lr", type=float, default=0.005, help="Initial learning rate")
    parser.add_argument("--output_dir", type=str, default="models", help="Directory to save the best model")
    parser.add_argument("--seed", type=int, default=42)

    args = parser.parse_args()
    set_seed(args.seed)

    os.makedirs(args.output_dir, exist_ok=True)
    print(f"DEVICE is set to: {DEVICE}")
    print(f"è¨“ç·´åƒæ•¸: Epochs={args.epochs}, Batch Size={args.batch_size}, LR={args.lr}")

    # --- 2. æº–å‚™è³‡æ–™ ---
    DATA_ROOT = args.data_root
    gt_path = os.path.join(DATA_ROOT, "train", "gt.txt")
    img_dir = os.path.join(DATA_ROOT, "train", "img")

    if not os.path.isfile(gt_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ¨™è¨»æª”ï¼š{gt_path}")
    if not os.path.isdir(img_dir):
        raise NotADirectoryError(f"æ‰¾ä¸åˆ°å½±åƒè³‡æ–™å¤¾ï¼š{img_dir}")

    full_annotations = pd.read_csv(gt_path, header=None, names=["frame", "bb_left", "bb_top", "bb_width", "bb_height"])

    # data analysis code (optional)
    """
    print("--- é–‹å§‹æ•¸æ“šæ¢ç´¢ ---")

    # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
    output_dir = "data_analysis"  # ä½ å¯ä»¥è‡ªè¨‚è³‡æ–™å¤¾åç¨±
    os.makedirs(output_dir, exist_ok=True)

    # è¨ˆç®—é¢ç©å’Œé•·å¯¬æ¯”
    full_annotations["area"] = full_annotations["bb_width"] * full_annotations["bb_height"]
    full_annotations["aspect_ratio"] = full_annotations["bb_width"] / (full_annotations["bb_height"] + 1e-6)

    # --- 1. ç¹ªè£½ä¸¦å„²å­˜é¢ç©çš„ç›´æ–¹åœ– ---
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(full_annotations["area"], bins=50, kde=True)
    plt.title("Bbox é¢ç©åˆ†ä½ˆ")
    plt.xlabel("é¢ç© (pixels)")
    plt.ylabel("æ•¸é‡")
    plt.yscale("log")

    # --- 2. ç¹ªè£½ä¸¦å„²å­˜é•·å¯¬æ¯”çš„ç›´æ–¹åœ– ---
    plt.subplot(1, 2, 2)
    sns.histplot(full_annotations["aspect_ratio"], bins=50, kde=True)
    plt.title("Bbox é•·å¯¬æ¯”åˆ†ä½ˆ")
    plt.xlabel("é•·å¯¬æ¯” (å¯¬/é«˜)")
    plt.xlim(0, 10)
    plt.ylabel("æ•¸é‡")
    plt.yscale("log")

    # --- !! é—œéµä¿®æ”¹ !! ---
    # å°‡ plt.show() æ”¹ç‚º plt.savefig()
    plt.tight_layout()
    save_path = os.path.join(output_dir, "bbox_distribution.png")
    plt.savefig(save_path)
    print(f"âœ… åœ–è¡¨å·²å„²å­˜è‡³: {save_path}")

    # æ¸…é™¤ç•¶å‰çš„åœ–å½¢ï¼Œä»¥é˜²è¬ä¸€
    plt.close()

    # --- 3. æ‰“å°çµ±è¨ˆæ•¸æ“šåˆ°çµ‚ç«¯æ©Ÿ ---
    print("\né¢ç©çµ±è¨ˆæ•¸æ“š (Area Stats):")
    print(full_annotations["area"].describe())
    print("\né•·å¯¬æ¯”çµ±è¨ˆæ•¸æ“š (Aspect Ratio Stats):")
    print(full_annotations["aspect_ratio"].describe())

    print("\n--- æ•¸æ“šæ¢ç´¢å®Œæˆï¼Œç¨‹å¼å³å°‡é€€å‡º ---")
    sys.exit()  # åˆ†æå®Œç•¢ï¼Œé€€å‡ºç¨‹å¼
    """

    # =================================================================
    # âœ¨âœ¨âœ¨ Data cleaning âœ¨âœ¨âœ¨
    # =================================================================
    print(f"åŸå§‹æ¨™è¨»æ•¸é‡: {len(full_annotations)}")

    # 1. æ ¹æ“šé¢ç©çµ±è¨ˆæ•¸æ“šï¼Œéæ¿¾æ‰é¢ç©å°æ–¼ 500 çš„ Bbox
    MIN_AREA = 500
    full_annotations["area"] = full_annotations["bb_width"] * full_annotations["bb_height"]
    full_annotations = full_annotations[full_annotations["area"] > MIN_AREA]
    print(f"éæ¿¾æ‰é¢ç©éå° Bbox å¾Œçš„æ•¸é‡: {len(full_annotations)}")

    # 2. æ ¹æ“šé•·å¯¬æ¯”çµ±è¨ˆæ•¸æ“šï¼Œéæ¿¾æ‰å½¢ç‹€ç•¸å½¢çš„ Bbox
    MAX_ASPECT_RATIO = 6.0
    full_annotations["aspect_ratio"] = full_annotations["bb_width"] / (full_annotations["bb_height"] + 1e-6)
    full_annotations = full_annotations[
        (full_annotations["aspect_ratio"] < MAX_ASPECT_RATIO)
        & (full_annotations["aspect_ratio"] > 1 / MAX_ASPECT_RATIO)
    ]
    print(f"éæ¿¾æ‰ç•¸å½¢ Bbox å¾Œçš„æ•¸é‡: {len(full_annotations)}")

    # ç§»é™¤è¼”åŠ©æ¬„ä½ï¼Œä¿æŒ DataFrame ä¹¾æ·¨
    full_annotations = full_annotations.drop(columns=["area", "aspect_ratio"])
    # ==================================================================

    # âœ… æ›´ç©©å¥çš„æª”åè§£æï¼ˆåªæ”¶ç´”æ•¸å­—æª”åï¼Œå¦‚ 00000001.jpgï¼‰
    existing_files = set()
    for f in os.listdir(img_dir):
        stem, _ = os.path.splitext(f)
        if stem.isdigit():
            existing_files.add(int(stem))

    annotated_frames = set(map(int, full_annotations["frame"].unique()))
    valid_frames = sorted(existing_files.intersection(annotated_frames))

    if len(valid_frames) < 2:
        raise RuntimeError("å¯ç”¨å½±åƒä¸è¶³ä»¥åˆ‡åˆ† train/valï¼Œè«‹æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§ã€‚")

    # å›ºå®šéš¨æ©Ÿç¨®å­å¾Œå† shuffleï¼Œç¢ºä¿å¯é‡ç¾
    rng = random.Random(args.seed)
    rng.shuffle(valid_frames)

    split_point = int(0.8 * len(valid_frames))
    # è‡³å°‘ç•™ 1 å¼µçµ¦é©—è­‰ï¼ˆä»¥å… 100%/0% é‚Šç•Œï¼‰
    split_point = min(max(1, split_point), len(valid_frames) - 1)

    train_frames = valid_frames[:split_point]
    val_frames = valid_frames[split_point:]

    train_dataset = PigDataset(
        root_dir=DATA_ROOT,
        frame_ids=train_frames,
        is_train=True,  # éœ€è¦æ¨™è¨»
        transforms=get_transform(train=True),
    )
    val_dataset = PigDataset(
        root_dir=DATA_ROOT,
        frame_ids=val_frames,
        is_train=True,  # é©—è­‰é›†ä»å–è‡ª trainï¼Œæœ‰æ¨™è¨» â†’ True
        transforms=get_transform(train=False),  # é©—è­‰ç¦ç”¨éš¨æ©Ÿå¢å¼·
    )

    # --- DataLoaderï¼ˆå¿«åˆç©©ï¼‰ ---
    cpu_cnt = os.cpu_count() or 2
    num_workers = max(1, cpu_cnt - 1)  # è‡³å°‘ 1ï¼ŒColab/é›²ç«¯é€šå¸¸é€™æ¨£æœ€ç©©
    g = torch.Generator()
    g.manual_seed(args.seed)

    dl_kwargs = dict(
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,  # âœ… ä½¿ç”¨åŒ¯å…¥çš„å‡½å¼ï¼Œä¸è¦å¯« utils.collate_fn
        worker_init_fn=seed_worker,
        generator=g,
    )
    if num_workers > 0:
        dl_kwargs["persistent_workers"] = True
        dl_kwargs["prefetch_factor"] = 2

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, **dl_kwargs)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, **dl_kwargs)

    print(f"è¨“ç·´é›†å¤§å°: {len(train_dataset)}, é©—è­‰é›†å¤§å°: {len(val_dataset)}")

    # --- 3. å»ºç«‹æ¨¡å‹ã€å„ªåŒ–å™¨èˆ‡å­¸ç¿’ç‡æ’ç¨‹å™¨ ---
    model = create_model(NUM_CLASSES)
    model.to(DEVICE)

    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=args.lr, momentum=0.9, weight_decay=0.0005)

    # ä½¿ç”¨ CosineAnnealingLR è®“å­¸ç¿’ç‡å¹³æ»‘ä¸‹é™
    lr_scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=0)

    # --- 4. è¨“ç·´èˆ‡é©—è­‰è¿´åœˆ ---
    best_map = -1.0
    best_path = os.path.join(args.output_dir, "best_model.pth")
    log_file_path = os.path.join(args.output_dir, "training_log.csv")  # âœ… æ—¥èªŒæ”¾åœ¨ output_dir

    with open(log_file_path, mode="w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Epoch", "mAP_50:95", "AP_50"])

    print("\n--- é–‹å§‹è¨“ç·´ ---")
    for epoch in range(args.epochs):
        train_one_epoch(model, optimizer, train_loader, DEVICE, epoch)
        lr_scheduler.step()

        coco_evaluator = evaluate(model, val_loader, DEVICE)
        current_map = coco_evaluator.coco_eval["bbox"].stats[0]  # mAP_50:95
        current_ap50 = coco_evaluator.coco_eval["bbox"].stats[1]  # AP_50

        with open(log_file_path, mode="a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([epoch + 1, f"{current_map:.4f}", f"{current_ap50:.4f}"])

        if current_map > best_map:
            best_map = current_map
            torch.save(model.state_dict(), best_path)
            print(f"ğŸ‰ New best model saved to {best_path} with mAP: {best_map:.4f} at epoch {epoch + 1}")

    print("\n--- è¨“ç·´å®Œæˆ ---")
    print(f"æ•´å€‹è¨“ç·´éç¨‹ä¸­æœ€å¥½çš„ mAP åˆ†æ•¸æ˜¯: {best_map:.4f}, å°æ‡‰æ¨¡å‹å·²å„²å­˜è‡³ {best_path}")


if __name__ == "__main__":
    main()
