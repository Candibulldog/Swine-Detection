# src/train.py

import argparse
import csv
import os
import random
import shutil
from pathlib import Path

import numpy as np
import pandas as pd
import torch
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.utils.data import DataLoader

from src.dataset import PigDataset
from src.engine import evaluate, train_one_epoch
from src.model import create_model
from src.transforms import get_transform
from src.utils import collate_fn

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
NUM_CLASSES = 2  # 1 (pig) + 1 (background)


def set_seed(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
        # ç¢ºä¿ cuDNN çš„ç¢ºå®šæ€§ï¼Œå¯èƒ½ç¨å¾®å½±éŸ¿æ•ˆèƒ½ï¼Œä½†å¯é‡ç¾æ€§æ›´é«˜
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False


def seed_worker(_):
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


# è³‡æ–™æ¸…æ´—ï¼Œç§»é™¤ç„¡æ•ˆçš„ bounding box æ¨™è¨»
def filter_annotations(df: pd.DataFrame) -> pd.DataFrame:
    print(f"åŸå§‹æ¨™è¨»æ•¸é‡: {len(df)}")
    initial_count = len(df)
    df_filtered = df[(df["bb_width"] > 1) & (df["bb_height"] > 1)].copy()
    removed_count = initial_count - len(df_filtered)
    if removed_count > 0:
        print(f"ç§»é™¤éæ³•æ¨™è¨» (w/h <= 1): {removed_count} å€‹")
    print(f"éæ¿¾å¾Œçš„æ¨™è¨»æ•¸é‡: {len(df_filtered)}")
    return df_filtered


def main():
    parser = argparse.ArgumentParser(description="Pig Detection Training Script")
    parser.add_argument("--data_root", type=Path, default=Path("./data"), help="Root path containing train/ and test/")
    parser.add_argument("--epochs", type=int, default=30, help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=4, help="Batch size for training")
    parser.add_argument("--lr", type=float, default=0.005, help="Initial learning rate for AdamW")
    parser.add_argument("--output_dir", type=Path, default=Path("models"), help="Directory to save the best model")
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    set_seed(args.seed)
    args.output_dir.mkdir(exist_ok=True)

    # åœ¨æ—¥èªŒæ–‡ä»¶åä¸­åŒ…å« seedï¼Œæ–¹ä¾¿å€åˆ†ä¸åŒå¯¦é©—
    log_filename = f"training_log_seed_{args.seed}.csv"
    log_path = args.output_dir / log_filename

    print(f"DEVICE is set to: {DEVICE}")
    print(f"è¨“ç·´åƒæ•¸: {vars(args)}")

    # --- 1. æº–å‚™è³‡æ–™ ---
    gt_path = args.data_root / "train" / "gt.txt"
    img_dir = args.data_root / "train" / "img"

    full_annotations = pd.read_csv(gt_path, header=None, names=["frame", "bb_left", "bb_top", "bb_width", "bb_height"])

    # âœ¨ åŸ·è¡Œè³‡æ–™æ¸…æ´—
    annotations = filter_annotations(full_annotations)

    # æ‰¾å‡ºå¯¦éš›å­˜åœ¨ä¸”æœ‰æ¨™è¨»çš„åœ–ç‰‡ frames
    existing_files = {int(p.stem) for p in img_dir.glob("*.jpg") if p.stem.isdigit()}
    annotated_frames = set(map(int, annotations["frame"].unique()))
    valid_frames = sorted(list(existing_files.intersection(annotated_frames)))

    if len(valid_frames) < 2:
        raise RuntimeError("å¯ç”¨å½±åƒä¸è¶³ä»¥åˆ‡åˆ† train/valï¼Œè«‹æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§ã€‚")

    # å¯é‡ç¾çš„ train/val åˆ‡åˆ†
    rng = random.Random(args.seed)
    rng.shuffle(valid_frames)
    split_point = int(0.8 * len(valid_frames))
    train_frames = valid_frames[:split_point]
    val_frames = valid_frames[split_point:]

    train_dataset = PigDataset(args.data_root, train_frames, is_train=True, transforms=get_transform(train=True))
    val_dataset = PigDataset(args.data_root, val_frames, is_train=True, transforms=get_transform(train=False))

    # --- 2. å»ºç«‹ DataLoader ---
    # ä¼ºæœå™¨ç’°å¢ƒä¸‹å¯é©åº¦å¢åŠ  num_workers
    num_workers = min(int(os.cpu_count() * 0.75), 12)
    g = torch.Generator().manual_seed(args.seed)

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,
        worker_init_fn=seed_worker,
        generator=g,
        persistent_workers=num_workers > 0,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=collate_fn,
        worker_init_fn=seed_worker,
        generator=g,
        persistent_workers=num_workers > 0,
    )
    print(f"è¨“ç·´é›†: {len(train_dataset)} | é©—è­‰é›†: {len(val_dataset)} | DataLoader Workers: {num_workers}")

    # --- 3. å»ºç«‹æ¨¡å‹èˆ‡å„ªåŒ–å™¨ ---
    model = create_model(NUM_CLASSES).to(DEVICE)
    params = [p for p in model.parameters() if p.requires_grad]

    # âœ¨ ä½¿ç”¨ AdamW å„ªåŒ–å™¨
    optimizer = torch.optim.AdamW(params, lr=args.lr, weight_decay=0.0005)
    lr_scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=0)

    # --- 4. è¨“ç·´è¿´åœˆ ---
    best_map = -1.0
    best_model_filename = f"best_model_seed_{args.seed}.pth"
    best_path = args.output_dir / best_model_filename

    # set of epochs to save intermediate checkpoints
    checkpoint_epochs = {40, 80, 120}

    with open(log_path, mode="w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["Epoch", "mAP_50:95", "AP_50", "Seed"])

    print("\n--- é–‹å§‹è¨“ç·´ ---")
    for epoch in range(args.epochs):
        train_one_epoch(model, optimizer, train_loader, DEVICE, epoch)
        lr_scheduler.step()

        coco_evaluator = evaluate(model, val_loader, DEVICE)
        # coco_evaluator.coco_eval['bbox'].stats æ˜¯ä¸€å€‹ numpy array
        # [mAP@.5:.95, AP@.5, AP@.75, mAP_small, mAP_medium, mAP_large, ...]
        stats = coco_evaluator.coco_eval["bbox"].stats
        current_map = stats[0]
        current_ap50 = stats[1]

        with open(log_path, mode="a", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([epoch + 1, f"{current_map:.4f}", f"{current_ap50:.4f}", args.seed])

        if current_map > best_map:
            best_map = current_map
            torch.save(model.state_dict(), best_path)
            print(f"ğŸ‰ New best model saved to {best_path} with mAP: {best_map:.4f} at epoch {epoch + 1}")

        # åœ¨ç‰¹å®š epoch ä¿å­˜ checkpoint modelï¼Œé¿å… overfitting
        if (epoch + 1) in checkpoint_epochs:
            if best_path.exists():
                checkpoint_path = args.output_dir / f"best_model_seed_{args.seed}_upto_epoch_{epoch + 1}.pth"
                shutil.copy2(best_path, checkpoint_path)
                print(f"âœ… Checkpoint saved: Current best model copied to {checkpoint_path}")

    print(f"\n--- è¨“ç·´å®Œæˆ ---\nBest mAP: {best_map:.4f} saved at {best_path}")


if __name__ == "__main__":
    main()
